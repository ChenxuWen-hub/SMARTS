(huawei-smart) bit@bit:~/SMARTS/competition/track1/train$ python run.py 

Torch cuda is available:  True 


Logdir: /home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43 

Waiting on /home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/smarts/scenarios/intersection/1_to_2lane_left_turn_c/. ...
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/gym/logger.py:34: UserWarning: WARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize("%s: %s" % ("WARN", msg % args), "yellow"))
Waiting on /home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/smarts/scenarios/intersection/1_to_2lane_left_turn_c/. ...

Start training.

Using cuda device
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=8 and n_envs=1)
  f"You have specified a mini-batch size of {batch_size},"
？？？？？是不是一直在for index in range(config[epochs]):里循环???epoch:0

Training on 1_to_2lane_left_turn_c.

####
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': pil_image.HAMMING,
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': pil_image.BOX,
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': pil_image.LANCZOS,
Logging to /home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43/tensorboard/PPO_1
?????????????????????有没有在这里发生循环？num_timesteps: 0
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 1
reward: {'Agent_0': 0.9622502356910827}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502356910827. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.9357189628976812e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.9357189628976812e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555558286464084}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555558286464084. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622507013523647}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622507013523647. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.9622502326965225}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965225. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.935719051715523e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.935719051715523e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.111111, -2.0, -2.0, -2.0, -10.0, -2.0, -2.0, -2.0]
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 5          |
|    ep_rew_mean     | -14.888889 |
| time/              |            |
|    fps             | 1          |
|    iterations      | 1          |
|    time_elapsed    | 4          |
|    total_timesteps | 8          |
-----------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 8
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622507095336887}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622507095336887. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.935719318169049e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.935719318169049e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.111111161098469}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.111111161098469. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 2.2351741790771484e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 2.2351741790771484e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 2.2351741790771484e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 2.2351741790771484e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.11111068725587}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.11111068725587. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.962250712528224}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.962250712528224. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -10.0, 0.96225053]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.5         |
|    ep_rew_mean          | -21.444445  |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 16          |
| train/                  |             |
|    approx_kl            | 0.020044468 |
|    clip_fraction        | 0.537       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.00471     |
|    learning_rate        | 0.0003      |
|    loss                 | 13.6        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 78.7        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 16
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111111610984459}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984459}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 1
reward: {'Agent_0': 1.1111111640930194}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111640930194}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 2
reward: {'Agent_0': 0.9622502356910871}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910871}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 3
reward: {'Agent_0': 1.9357187852619973e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.9357187852619973e-08}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.9357188e-08]
这一步的reward[0]: 1.9357187852619973e-08
n_steps: 4
reward: {'Agent_0': 0.555555351809252}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.555555351809252. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.1111112, 1.1111112, 0.96225023, 1.9357188e-08, -10.0, -2.0, -2.0, -10.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8           |
|    ep_rew_mean          | -20.914055  |
| time/                   |             |
|    fps                  | 4           |
|    iterations           | 3           |
|    time_elapsed         | 5           |
|    total_timesteps      | 24          |
| train/                  |             |
|    approx_kl            | 0.049600184 |
|    clip_fraction        | 0.613       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0291      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.03        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0619     |
|    value_loss           | 20.8        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 24
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.9357179859014195e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.9357179859014195e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.5555558286463995}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555558286463995. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555558204650852}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555558204650852. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.5555553518092431}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553518092431. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.5555558204650834}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555558204650834. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -2.0, -10.0, -2.0, -2.0, -10.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.2        |
|    ep_rew_mean          | -18.548433 |
| time/                   |            |
|    fps                  | 5          |
|    iterations           | 4          |
|    time_elapsed         | 6          |
|    total_timesteps      | 32         |
| train/                  |            |
|    approx_kl            | 0.07945799 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.4        |
|    learning_rate        | 0.0003     |
|    loss                 | 8.75       |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 62.8       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 32
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.9357179859014195e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.9357179859014195e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622507095336754}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622507095336754. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622502326965412}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965412. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.555555351809252}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.555555351809252. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 5
reward: {'Agent_0': 0.9622502326965172}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502326965172}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 6
reward: {'Agent_0': 1.1111111610984636}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984636}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 1.1111111640930176}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111640930176}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -2.0, -10.0, 0.96225053, 0.96225023, 1.1111112, 1.1111112]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6           |
|    ep_rew_mean          | -18.457027  |
| time/                   |             |
|    fps                  | 6           |
|    iterations           | 5           |
|    time_elapsed         | 6           |
|    total_timesteps      | 40          |
| train/                  |             |
|    approx_kl            | 0.037443243 |
|    clip_fraction        | 0.425       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | -2.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0654     |
|    value_loss           | 72.1        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 40
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502356910818}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910818}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 1
reward: {'Agent_0': 0.962250709533718}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250709533718}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 2
reward: {'Agent_0': 0.5555553518092253}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553518092253. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502326965225}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965225. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.5555553518092529}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553518092529. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225023, 0.9622507, -2.0, -10.0, -2.0, -2.0, -10.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.88        |
|    ep_rew_mean          | -16.333868  |
| time/                   |             |
|    fps                  | 6           |
|    iterations           | 6           |
|    time_elapsed         | 6           |
|    total_timesteps      | 48          |
| train/                  |             |
|    approx_kl            | 0.115350656 |
|    clip_fraction        | 0.45        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | -0.064      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.66        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.071      |
|    value_loss           | 7.26        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 48
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502326965225}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965225. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.5555553518092529}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553518092529. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 3
reward: {'Agent_0': 1.1111111610984459}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984459}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 1.1111111640930194}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111640930194}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 5
reward: {'Agent_0': 0.9622502356910871}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910871}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 6
reward: {'Agent_0': 1.1111111610984636}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984636}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 0.9622507125282347}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282347}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, 0.96225053, 1.1111112, 1.1111112, 0.96225023, 1.1111112, 0.9622507]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.56        |
|    ep_rew_mean          | -16.074549  |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 7           |
|    time_elapsed         | 7           |
|    total_timesteps      | 56          |
| train/                  |             |
|    approx_kl            | 0.002723597 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.366       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 1.56        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 56
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111106842613214}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111106842613214}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111107]
这一步的reward[0]: 1.1111106872558594
n_steps: 1
reward: {'Agent_0': 1.111110687255863}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111110687255863}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111107]
这一步的reward[0]: 1.1111106872558594
n_steps: 2
reward: {'Agent_0': 0.9622507125282365}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282365}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 3
reward: {'Agent_0': 0.9622507095336914}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622507095336914. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 1.111110684261316}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.111110684261316. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.962250712528256}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250712528256}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 6
reward: {'Agent_0': 0.9622507095336914}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507095336914}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 7
reward: {'Agent_0': 0.5555553518092147}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553518092147. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.1111107, 1.1111107, 0.9622507, -10.0, -10.0, 0.9622507, 0.9622507, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.56        |
|    ep_rew_mean          | -16.074549  |
| time/                   |             |
|    fps                  | 8           |
|    iterations           | 8           |
|    time_elapsed         | 7           |
|    total_timesteps      | 64          |
| train/                  |             |
|    approx_kl            | 0.058952197 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.202      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.26        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 13.8        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 64
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622516550267015}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622516550267015. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.555555351809236. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 3
reward: {'Agent_0': 0.9622502356910854}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910854}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 4
reward: {'Agent_0': 0.962250232696535}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250232696535}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 5
reward: {'Agent_0': 1.111111161098461}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111111161098461}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.96225071252824}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.96225071252824. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.9622507095336914}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622507095336914. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, 1.111111, 0.96225023, 0.96225023, 1.1111112, -10.0, -10.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.6         |
|    ep_rew_mean          | -16.734188  |
| time/                   |             |
|    fps                  | 8           |
|    iterations           | 9           |
|    time_elapsed         | 8           |
|    total_timesteps      | 72          |
| train/                  |             |
|    approx_kl            | 0.005110085 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.056       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.9        |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00771     |
|    value_loss           | 132         |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 72
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.555555351809268}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555351809268. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.5555553436279297}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553436279297. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622497476780545}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622497476780545. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1111106842613037}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111106842613037. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 1.1111106872558594}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111106872558594. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622497588539343}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622497588539343. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622502326965225}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965225. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -2.0, -2.0, -2.0, -10.0, -2.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.09        |
|    ep_rew_mean          | -18.472288  |
| time/                   |             |
|    fps                  | 9           |
|    iterations           | 10          |
|    time_elapsed         | 8           |
|    total_timesteps      | 80          |
| train/                  |             |
|    approx_kl            | 0.018542677 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.578      |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 72.4        |
-----------------------------------------
Early stopping at step 8 due to reaching max kl: 0.17
?????????????????????有没有在这里发生循环？num_timesteps: 80
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502326965359}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965359. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622502326965332}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622502326965332. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 3
reward: {'Agent_0': 1.1111111610984459}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984459}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622502356910942}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910942}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 5
reward: {'Agent_0': 0.9622502326965305}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622502326965305. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.9622502326965359}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622502326965359. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.5555553518092573}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553518092573. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, 0.96225053, 1.1111112, 0.96225023, -10.0, -10.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.83       |
|    ep_rew_mean          | -18.266264 |
| time/                   |            |
|    fps                  | 10         |
|    iterations           | 11         |
|    time_elapsed         | 8          |
|    total_timesteps      | 88         |
| train/                  |            |
|    approx_kl            | 0.1735977  |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.58       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0879    |
|    value_loss           | 13.8       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 88
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.5555562749505203}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555562749505203. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1111111610984539}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984539}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622502356910916}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910916}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 5
reward: {'Agent_0': 1.111111161098461}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111111161098461}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622502356910925}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910925}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 7
reward: {'Agent_0': 0.5555553518092573}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555553518092573}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, -2.0, 1.1111112, 0.96225023, 1.1111112, 0.96225023, 0.55555534]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.92       |
|    ep_rew_mean          | -19.243042 |
| time/                   |            |
|    fps                  | 10         |
|    iterations           | 12         |
|    time_elapsed         | 9          |
|    total_timesteps      | 96         |
| train/                  |            |
|    approx_kl            | 0.03683822 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.904     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.1       |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 72         |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 96
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1175860237244706e-08}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.117586e-08]
这一步的reward[0]: 1.1175860237244706e-08
n_steps: 1
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.5555562749505203}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555562749505203. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555558204650879}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555558204650879. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 5
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 6
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.117586e-08, -2.0, -2.0, -10.0, 0.96225053, 0.55555534, -10.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.14        |
|    ep_rew_mean          | -18.67552   |
| time/                   |             |
|    fps                  | 10          |
|    iterations           | 13          |
|    time_elapsed         | 9           |
|    total_timesteps      | 104         |
| train/                  |             |
|    approx_kl            | 0.040891968 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00376     |
|    value_loss           | 27.7        |
-----------------------------------------
？？？？？是不是一直在for index in range(config[epochs]):里循环???epoch:1

Training on 1_to_2lane_left_turn_c.

####
Logging to /home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43/tensorboard/PPO_2
?????????????????????有没有在这里发生循环？num_timesteps: 0
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 1
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 2
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555558204650852}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555558204650852. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.5555558204650897}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555558204650897. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1175861125423125e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175861125423125e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 7
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225053, 0.55555534, -10.0, -2.0, -2.0, -10.0, 0.96225053, 0.55555534]
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 6          |
|    ep_rew_mean     | -22.482193 |
| time/              |            |
|    fps             | 17         |
|    iterations      | 1          |
|    time_elapsed    | 0          |
|    total_timesteps | 8          |
-----------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 8
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.1175857572709447e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175857572709447e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.5555553518092551}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553518092551. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 6
reward: {'Agent_0': 0.9622502356910854}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910854}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 7
reward: {'Agent_0': 0.5555553518092529}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555553518092529}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, -2.0, -10.0, 1.111111, 0.96225023, 0.55555534]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.5         |
|    ep_rew_mean          | -23.482193  |
| time/                   |             |
|    fps                  | 19          |
|    iterations           | 2           |
|    time_elapsed         | 0           |
|    total_timesteps      | 16          |
| train/                  |             |
|    approx_kl            | 0.023748934 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | -0.751      |
|    learning_rate        | 0.0003      |
|    loss                 | 20.2        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 53.5        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 16
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175858460887866e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175858460887866e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 1.1175858460887866e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175858460887866e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.5555553212762083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 5
reward: {'Agent_0': 0.9622502356910854}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910854}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 6
reward: {'Agent_0': 1.9357186964441553e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.9357186964441553e-08}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.9357188e-08]
这一步的reward[0]: 1.9357187852619973e-08
n_steps: 7
reward: {'Agent_0': 0.962250232696535}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250232696535}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, -10.0, 1.111111, 0.96225023, 1.9357188e-08, 0.96225023]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.67       |
|    ep_rew_mean          | -22.778488 |
| time/                   |            |
|    fps                  | 20         |
|    iterations           | 3          |
|    time_elapsed         | 1          |
|    total_timesteps      | 24         |
| train/                  |            |
|    approx_kl            | 0.12378474 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.754     |
|    explained_variance   | -0.717     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.94       |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 27.2       |
----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.17
?????????????????????有没有在这里发生循环？num_timesteps: 24
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502326965305}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622502326965305. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.9622507095336861}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622507095336861. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.9622507095336914}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622507095336914. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.9622507095336861}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622507095336861. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.9622507095336914}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622507095336914. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622507095336958}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622507095336958. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.5555553518092644}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553518092644. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -10.0, -10.0, -10.0, -2.0, -2.0, -10.0, 0.96225053]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.75       |
|    ep_rew_mean          | -29.824963 |
| time/                   |            |
|    fps                  | 20         |
|    iterations           | 4          |
|    time_elapsed         | 1          |
|    total_timesteps      | 32         |
| train/                  |            |
|    approx_kl            | 0.16871314 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.939     |
|    explained_variance   | 0.145      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.55       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 11.6       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 32
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 1
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.5555553436279288}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553436279288. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 6
reward: {'Agent_0': 1.1111111640930131}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111640930131}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 1.1111111640930194}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111640930194}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.55555534, -10.0, -2.0, -2.0, -10.0, 1.111111, 1.1111112, 1.1111112]
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.4          |
|    ep_rew_mean          | -28.35641    |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 5            |
|    time_elapsed         | 1            |
|    total_timesteps      | 40           |
| train/                  |              |
|    approx_kl            | 0.0040566176 |
|    clip_fraction        | 0.3          |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.85        |
|    explained_variance   | 0.0452       |
|    learning_rate        | 0.0003       |
|    loss                 | 74           |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.0509       |
|    value_loss           | 211          |
------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 40
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502356910871}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910871}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 1
reward: {'Agent_0': 0.9622507095336861}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507095336861}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 2
reward: {'Agent_0': 1.111110684261316}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111110684261316}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111107]
这一步的reward[0]: 1.1111106872558594
n_steps: 3
reward: {'Agent_0': 0.9622507125282347}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622507125282347. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555553518092626}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.5555553518092626. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 1.1175865566315224e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175865566315224e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.555556274950515}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555556274950515. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622507013524011}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622507013524011. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225023, 0.9622507, 1.1111107, -10.0, -10.0, -2.0, -2.0, -10.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8           |
|    ep_rew_mean          | -28.235518  |
| time/                   |             |
|    fps                  | 21          |
|    iterations           | 6           |
|    time_elapsed         | 2           |
|    total_timesteps      | 48          |
| train/                  |             |
|    approx_kl            | 0.038972497 |
|    clip_fraction        | 0.6         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.23        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0307     |
|    value_loss           | 10.1        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 48
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 1
reward: {'Agent_0': 0.9622502356910827}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502356910827. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.1111111610984654}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984654}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 3
reward: {'Agent_0': 1.1111111640930176}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111640930176}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.96225071252824}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.96225071252824}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 5
reward: {'Agent_0': 0.5555553518092573}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555553518092573}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 6
reward: {'Agent_0': 0.5555553436279297}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.5555553436279297. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.111111, -2.0, 1.1111112, 1.1111112, 0.9622507, 0.55555534, -10.0, -10.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 8          |
|    ep_rew_mean          | -28.235518 |
| time/                   |            |
|    fps                  | 22         |
|    iterations           | 7          |
|    time_elapsed         | 2          |
|    total_timesteps      | 56         |
| train/                  |            |
|    approx_kl            | 0.05935678 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.864     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.6       |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 30.7       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 56
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 1.1111109891533877}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111109891533877}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 3
reward: {'Agent_0': 0.9622502356910854}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910854}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 4
reward: {'Agent_0': 0.962250232696535}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250232696535}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 5
reward: {'Agent_0': 0.5555553518092502}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.5555553518092502. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 1.1175862013601545e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862013601545e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, 1.111111, 0.96225023, 0.96225023, -10.0, -2.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 8.29       |
|    ep_rew_mean          | -28.365995 |
| time/                   |            |
|    fps                  | 22         |
|    iterations           | 8          |
|    time_elapsed         | 2          |
|    total_timesteps      | 64         |
| train/                  |            |
|    approx_kl            | 0.09613931 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.814     |
|    explained_variance   | -0.405     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.896      |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0313    |
|    value_loss           | 7.81       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 64
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175861125423125e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175861125423125e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 2
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 3
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0, 1.1111112, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.12        |
|    ep_rew_mean          | -27.440794  |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 9           |
|    time_elapsed         | 3           |
|    total_timesteps      | 72          |
| train/                  |             |
|    approx_kl            | 0.050215304 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.73        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 20          |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 72
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622506648302069}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506648302069. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.555555962756829}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555962756829. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555566251277915}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555566251277915. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.962250760957045}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.962250760957045. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 6
reward: {'Agent_0': 0.9622502326965172}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502326965172}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 7
reward: {'Agent_0': 0.9622502326965359}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.9622502326965359. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -10.0, -2.0, -10.0, 0.96225053, 0.96225023, -10.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.56        |
|    ep_rew_mean          | -28.76638   |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 10          |
|    time_elapsed         | 3           |
|    total_timesteps      | 80          |
| train/                  |             |
|    approx_kl            | 0.026936308 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.223       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.998       |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 80
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.555555351809252}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.555555351809252. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.5555553212762083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555553436279306}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553436279306. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.9622505375886208}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505375886208. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1111111610984539}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984539}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622502356910916}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502356910916}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 7
reward: {'Agent_0': 0.5555553518092502}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555553518092502}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, -10.0, -2.0, 1.1111112, 0.96225023, 0.55555534]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.4         |
|    ep_rew_mean          | -29.097294  |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 11          |
|    time_elapsed         | 3           |
|    total_timesteps      | 88          |
| train/                  |             |
|    approx_kl            | 0.022211552 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00855     |
|    value_loss           | 57.6        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 88
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175862013601545e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862013601545e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.5555553436279306}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553436279306. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1175857572709447e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175857572709447e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111111610984574. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.9622507125282667}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622507125282667. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.5555560670649573}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555560670649573. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.4         |
|    ep_rew_mean          | -29.097294  |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 12          |
|    time_elapsed         | 4           |
|    total_timesteps      | 96          |
| train/                  |             |
|    approx_kl            | 0.054970123 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 4.96        |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.15
?????????????????????有没有在这里发生循环？num_timesteps: 96
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 4.880130237694402e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 4.880130237694402e-07}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [4.8801303e-07]
这一步的reward[0]: 4.880130290985107e-07
n_steps: 1
reward: {'Agent_0': 0.5555546060204613}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555546060204613}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555546]
这一步的reward[0]: 0.5555546283721924
n_steps: 2
reward: {'Agent_0': 0.9622502245152038}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622502245152038}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225023]
这一步的reward[0]: 0.9622502326965332
n_steps: 3
reward: {'Agent_0': 1.1111111610984565}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984565}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.962250712528272}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250712528272}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 5
reward: {'Agent_0': 0.5555558286463764}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 0.5555558286463764. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 4.880130219930834e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 4.880130219930834e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.5555543676018893}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555543676018893. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [4.8801303e-07, 0.5555546, 0.96225023, 1.1111112, 0.9622507, -10.0, -10.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 8.4        |
|    ep_rew_mean          | -29.097294 |
| time/                   |            |
|    fps                  | 23         |
|    iterations           | 13         |
|    time_elapsed         | 4          |
|    total_timesteps      | 104        |
| train/                  |            |
|    approx_kl            | 0.15203124 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.667     |
|    explained_variance   | -0.881     |
|    learning_rate        | 0.0003     |
|    loss                 | 45.6       |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0337    |
|    value_loss           | 178        |
----------------------------------------
？？？？？是不是一直在for index in range(config[epochs]):里循环???epoch:2

Training on 1_to_2lane_left_turn_c.

####
Logging to /home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43/tensorboard/PPO_3
?????????????????????有没有在这里发生循环？num_timesteps: 0
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 1
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 2
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502326965323}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965323. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111110418891714}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111110418891714. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.1111110895872107}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111110895872107. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225053, 0.55555534, -10.0, -2.0, -2.0, -2.0, -2.0, -2.0]
---------------------------
| time/              |    |
|    fps             | 32 |
|    iterations      | 1  |
|    time_elapsed    | 0  |
|    total_timesteps | 8  |
---------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 8
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622506678247831}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506678247831. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.5555559478556678}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555559478556678. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 4
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 5
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | -32.482193  |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 2           |
|    time_elapsed         | 0           |
|    total_timesteps      | 16          |
| train/                  |             |
|    approx_kl            | 0.045722447 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.377      |
|    explained_variance   | -0.152      |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00898     |
|    value_loss           | 59.2        |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 16
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502326965323}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965323. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.1111110418891714}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111110418891714. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622506454730404}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506454730404. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555559553062475}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555559553062475. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 6.072223133202215e-07}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [6.072223e-07]
这一步的reward[0]: 6.07222318649292e-07
n_steps: 5
reward: {'Agent_0': 1.173466437798254e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.173466437798254e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555538684129839}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555538684129839. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622501872623008}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622501872623008. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -2.0, -2.0, 6.072223e-07, -10.0, -2.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 11         |
|    ep_rew_mean          | -32.482193 |
| time/                   |            |
|    fps                  | 29         |
|    iterations           | 3          |
|    time_elapsed         | 0          |
|    total_timesteps      | 24         |
| train/                  |            |
|    approx_kl            | 0.02543731 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.289     |
|    explained_variance   | 0.284      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.9       |
|    n_updates            | 280        |
|    policy_gradient_loss | 0.0372     |
|    value_loss           | 90.4       |
----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.22
?????????????????????有没有在这里发生循环？num_timesteps: 24
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111111610984583}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984583}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 1
reward: {'Agent_0': 0.9622507125282684}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282684}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 2
reward: {'Agent_0': 0.5555558286463782}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555558286463782}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555558]
这一步的reward[0]: 0.5555558204650879
n_steps: 3
reward: {'Agent_0': 7.26431599318289e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 7.26431599318289e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555541291833119}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555541291833119. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622499860966265}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622499860966265. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111110418891617}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891617}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 7
reward: {'Agent_0': 0.9622509136939321}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622509136939321. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.1111112, 0.9622507, 0.5555558, -10.0, -2.0, -2.0, 1.111111, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 11         |
|    ep_rew_mean          | -32.482193 |
| time/                   |            |
|    fps                  | 30         |
|    iterations           | 4          |
|    time_elapsed         | 1          |
|    total_timesteps      | 32         |
| train/                  |            |
|    approx_kl            | 0.21573275 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | -0.671     |
|    learning_rate        | 0.0003     |
|    loss                 | 56.1       |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 262        |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 32
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622509256005287}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622509256005287. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622508883476257}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622508883476257. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 3
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 4
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0, 1.1111112]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 17            |
|    ep_rew_mean          | -45.612183    |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 5             |
|    time_elapsed         | 1             |
|    total_timesteps      | 40            |
| train/                  |               |
|    approx_kl            | 0.00046008825 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0924       |
|    explained_variance   | 0.678         |
|    learning_rate        | 0.0003        |
|    loss                 | 4.74          |
|    n_updates            | 300           |
|    policy_gradient_loss | 0.00194       |
|    value_loss           | 83.3          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 40
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.555555947855666}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555947855666. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 1.2032687601859493e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.2032687601859493e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555538907647257}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555538907647257. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502245152038}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152038. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111111610984583}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111111610984583. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622507125282684}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622507125282684. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -10.0, -10.0, -2.0, -2.0, -2.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17         |
|    ep_rew_mean          | -45.612183 |
| time/                   |            |
|    fps                  | 28         |
|    iterations           | 6          |
|    time_elapsed         | 1          |
|    total_timesteps      | 48         |
| train/                  |            |
|    approx_kl            | 0.12823665 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.446     |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.31       |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0847    |
|    value_loss           | 25.4       |
----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.16
?????????????????????有没有在这里发生循环？num_timesteps: 48
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555558286463782}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555558286463782}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555558]
这一步的reward[0]: 0.5555558204650879
n_steps: 1
reward: {'Agent_0': 4.880130202167265e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 4.880130202167265e-07}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [4.8801303e-07]
这一步的reward[0]: 4.880130290985107e-07
n_steps: 2
reward: {'Agent_0': 0.5555543676018893}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555543676018893}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555544]
这一步的reward[0]: 0.5555543899536133
n_steps: 3
reward: {'Agent_0': 0.9622499860966265}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622499860966265}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225]
这一步的reward[0]: 0.9622499942779541
n_steps: 4
reward: {'Agent_0': 1.1111111610984494}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984494}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 5
reward: {'Agent_0': 1.1111111044883746}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111044883746}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 1.1111110895872116}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111110895872116. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.1111110597848892}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111110597848892. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.5555558, 4.8801303e-07, 0.5555544, 0.96225, 1.1111112, 1.1111112, -2.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17         |
|    ep_rew_mean          | -45.612183 |
| time/                   |            |
|    fps                  | 29         |
|    iterations           | 7          |
|    time_elapsed         | 1          |
|    total_timesteps      | 56         |
| train/                  |            |
|    approx_kl            | 0.15827136 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.469     |
|    explained_variance   | -1.76      |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 320        |
|    policy_gradient_loss | 0.0498     |
|    value_loss           | 565        |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 56
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111111640930158}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1111111640930158. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 2
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 3
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 1.1111111044883746}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1111111044883746. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0, 1.1111112, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | -48.099957  |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 8           |
|    time_elapsed         | 2           |
|    total_timesteps      | 64          |
| train/                  |             |
|    approx_kl            | 0.012038372 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | -195        |
|    learning_rate        | 0.0003      |
|    loss                 | 203         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 782         |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 64
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622506454730422}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506454730422. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622506573796272}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506573796272. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622506499290457}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622506499290457. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 4
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 5
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | -42.917736  |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 9           |
|    time_elapsed         | 2           |
|    total_timesteps      | 72          |
| train/                  |             |
|    approx_kl            | 0.004979074 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.0298      |
|    value_loss           | 37          |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 72
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555553436279288}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.5555553436279288. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 2
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 3
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0, 1.1111112, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 14.6       |
|    ep_rew_mean          | -38.830627 |
| time/                   |            |
|    fps                  | 26         |
|    iterations           | 10         |
|    time_elapsed         | 2          |
|    total_timesteps      | 80         |
| train/                  |            |
|    approx_kl            | 0.02520264 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.36       |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0272    |
|    value_loss           | 17         |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 80
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622506648302069}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506648302069. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622506946325293}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506946325293. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622506499290475}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622506499290475. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 4
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 5
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.5555553436279288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553436279288. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 13.8       |
|    ep_rew_mean          | -36.920704 |
| time/                   |            |
|    fps                  | 26         |
|    iterations           | 11         |
|    time_elapsed         | 3          |
|    total_timesteps      | 88         |
| train/                  |            |
|    approx_kl            | 0.06690871 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | 6.58       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0356    |
|    value_loss           | 30.4       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 88
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622502245152087}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622502245152087. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 2
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 3
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.9622502326965323}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502326965323. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622502326965341}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 0.9622502326965341. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.96225053, 0.55555534, -10.0, -2.0, -2.0, -2.0, -10.0]
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | -33.561073   |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 12           |
|    time_elapsed         | 3            |
|    total_timesteps      | 96           |
| train/                  |              |
|    approx_kl            | 0.0027550012 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.35         |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0197      |
|    value_loss           | 5.93         |
------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 96
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 1
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 2
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.555555947855666}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555947855666. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225053, 0.55555534, -10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 12          |
|    ep_rew_mean          | -33.561073  |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 13          |
|    time_elapsed         | 4           |
|    total_timesteps      | 104         |
| train/                  |             |
|    approx_kl            | 0.016829796 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 11.3        |
-----------------------------------------
？？？？？是不是一直在for index in range(config[epochs]):里循环???epoch:3

Training on 1_to_2lane_left_turn_c.

####
Logging to /home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43/tensorboard/PPO_4
?????????????????????有没有在这里发生循环？num_timesteps: 0
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 1
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 2
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1175860237244706e-08}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went off road. Penalty of off_road = 10. Agent reward: 1.1175860237244706e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 7
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225053, 0.55555534, -10.0, -2.0, -2.0, -10.0, 0.96225053, 0.55555534]
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 6          |
|    ep_rew_mean     | -22.482193 |
| time/              |            |
|    fps             | 18         |
|    iterations      | 1          |
|    time_elapsed    | 0          |
|    total_timesteps | 8          |
-----------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 8
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622506648302069}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622506648302069. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.555555962756829}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555962756829. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0, -2.0, -10.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6          |
|    ep_rew_mean          | -22.482193 |
| time/                   |            |
|    fps                  | 24         |
|    iterations           | 2          |
|    time_elapsed         | 0          |
|    total_timesteps      | 16         |
| train/                  |            |
|    approx_kl            | 0.11756217 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00371   |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.388      |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 16
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555544346571093}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555544346571093. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622501276576578}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622501276576578. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.1111111610984565}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984565}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 3
reward: {'Agent_0': 0.9622507125282684}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282684}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 4
reward: {'Agent_0': 0.5555558286463782}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555558286463782}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555558]
这一步的reward[0]: 0.5555558204650879
n_steps: 5
reward: {'Agent_0': 7.26431599318289e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 7.26431599318289e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555541291833119}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555541291833119. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622499860966265}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622499860966265. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, 1.1111112, 0.9622507, 0.5555558, -10.0, -2.0, -2.0]
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6           |
|    ep_rew_mean          | -22.482193  |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 3           |
|    time_elapsed         | 0           |
|    total_timesteps      | 24          |
| train/                  |             |
|    approx_kl            | 0.003261961 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 48.8        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000638   |
|    value_loss           | 139         |
-----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 24
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111110418891617}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891617}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 1
reward: {'Agent_0': 0.9622509136939321}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622509136939321. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.555556432143403}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555556432143403. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1138617939110418e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1138617939110418e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555534735322123}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555534735322123. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622498966896575}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622498966896575. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111111610984494}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984494}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 0.9622509509468387}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622509509468387}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225095]
这一步的reward[0]: 0.9622509479522705
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.111111, -2.0, -2.0, -10.0, -2.0, -2.0, 1.1111112, 0.96225095]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 6             |
|    ep_rew_mean          | -22.482193    |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 4             |
|    time_elapsed         | 1             |
|    total_timesteps      | 32            |
| train/                  |               |
|    approx_kl            | 0.00035710633 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.104        |
|    explained_variance   | 0.449         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.11          |
|    n_updates            | 420           |
|    policy_gradient_loss | 0.00397       |
|    value_loss           | 11.5          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 32
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555563054835311}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555563054835311}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555563]
这一步的reward[0]: 0.5555562973022461
n_steps: 1
reward: {'Agent_0': 1.2032687592977709e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.2032687592977709e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.5555531755089937}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555531755089937. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.9622497476780438}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622497476780438. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 1.1111110418891528}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891528}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 5
reward: {'Agent_0': 0.9622511744642459}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622511744642459. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.5555568866288176}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555568866288176. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.6503035986659143e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.6503035986659143e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.5555563, -10.0, -2.0, -2.0, 1.111111, -2.0, -2.0, -10.0]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 6             |
|    ep_rew_mean          | -22.482193    |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 5             |
|    time_elapsed         | 1             |
|    total_timesteps      | 40            |
| train/                  |               |
|    approx_kl            | 0.00032493472 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.125        |
|    explained_variance   | -1.76         |
|    learning_rate        | 0.0003        |
|    loss                 | 54.3          |
|    n_updates            | 430           |
|    policy_gradient_loss | 0.0053        |
|    value_loss           | 172           |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 40
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555524751544105}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555524751544105. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622495837652707}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622495837652707. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.1111110418891492}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891492}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 3
reward: {'Agent_0': 0.9622511893654071}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622511893654071}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622512]
这一步的reward[0]: 0.9622511863708496
n_steps: 4
reward: {'Agent_0': 0.5555567823206893}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555567823206893}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555568]
这一步的reward[0]: 0.5555567741394043
n_steps: 5
reward: {'Agent_0': 1.6801059210536096e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.6801059210536096e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555522218346773}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555522218346773. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.9622495092594612}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622495092594612. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, 1.111111, 0.9622512, 0.5555568, -10.0, -2.0, -2.0]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 6             |
|    ep_rew_mean          | -22.482193    |
| time/                   |               |
|    fps                  | 31            |
|    iterations           | 6             |
|    time_elapsed         | 1             |
|    total_timesteps      | 48            |
| train/                  |               |
|    approx_kl            | 0.00018693507 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.031        |
|    explained_variance   | 0.852         |
|    learning_rate        | 0.0003        |
|    loss                 | 37.8          |
|    n_updates            | 440           |
|    policy_gradient_loss | 0.00223       |
|    value_loss           | 137           |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 48
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111110418891457}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891457}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 1
reward: {'Agent_0': 0.9622514426851367}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622514426851367. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.5555573336636499}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555573336636499. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 2.18674540164443e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 2.18674540164443e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555514767766105}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555514767766105. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622493900501681}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622493900501681. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111110418891457}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891457}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 7
reward: {'Agent_0': 0.9622514277839755}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622514277839755}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622514]
这一步的reward[0]: 0.9622514247894287
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.111111, -2.0, -2.0, -10.0, -2.0, -2.0, 1.111111, 0.9622514]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 6             |
|    ep_rew_mean          | -22.482193    |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 7             |
|    time_elapsed         | 1             |
|    total_timesteps      | 56            |
| train/                  |               |
|    approx_kl            | 1.4901161e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000644     |
|    explained_variance   | -0.668        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.893         |
|    n_updates            | 450           |
|    policy_gradient_loss | 7.68e-05      |
|    value_loss           | 13.8          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 56
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555572591578439}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555572591578439}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555725]
这一步的reward[0]: 0.5555572509765625
n_steps: 1
reward: {'Agent_0': 2.156943075704021e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 2.156943075704021e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.5555512681603645}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555512681603645. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.9622492708408785}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622492708408785. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 1.1111110418891386}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111110418891386}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.111111]
这一步的reward[0]: 1.111111044883728
n_steps: 5
reward: {'Agent_0': 0.9622517034554505}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622517034554505. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.5555577881490663}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555577881490663. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 2.723187208175659e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 2.723187208175659e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.55555725, -10.0, -2.0, -2.0, 1.111111, -2.0, -2.0, -10.0]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6          |
|    ep_rew_mean          | -22.482193 |
| time/                   |            |
|    fps                  | 32         |
|    iterations           | 8          |
|    time_elapsed         | 1          |
|    total_timesteps      | 64         |
| train/                  |            |
|    approx_kl            | 0.0        |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.000148  |
|    explained_variance   | -0.824     |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 460        |
|    policy_gradient_loss | 1.59e-05   |
|    value_loss           | 77.2       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 64
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555504858493592}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555504858493592. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622490696752379}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622490696752379. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.111111161098421}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111111161098421}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 3
reward: {'Agent_0': 0.9622516662025475}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622516662025475}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225166]
这一步的reward[0]: 0.9622516632080078
n_steps: 4
reward: {'Agent_0': 0.5555577359950021}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Collided. Penalty of collisions = 10. Agent reward: 0.5555577359950021. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 6
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 7
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, 1.1111112, 0.96225166, -10.0, 0.96225053, 0.55555534, -10.0]
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 34.5      |
|    ep_rew_mean          | -87.40989 |
| time/                   |           |
|    fps                  | 31        |
|    iterations           | 9         |
|    time_elapsed         | 2         |
|    total_timesteps      | 72        |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.11e-05 |
|    explained_variance   | 0.269     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.8       |
|    n_updates            | 470       |
|    policy_gradient_loss | 1.18e-06  |
|    value_loss           | 46.1      |
---------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 72
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 3
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.555555947855666}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555947855666. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555544868111726}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555544868111726}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555545]
这一步的reward[0]: 0.5555545091629028
n_steps: 7
reward: {'Agent_0': 0.962250105305916}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250105305916}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622501]
这一步的reward[0]: 0.9622501134872437
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, 1.1111112, -2.0, -2.0, -10.0, 0.5555545, 0.9622501]
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 34.5         |
|    ep_rew_mean          | -87.40989    |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 10           |
|    time_elapsed         | 2            |
|    total_timesteps      | 80           |
| train/                  |              |
|    approx_kl            | 4.567206e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00315     |
|    explained_variance   | 0.435        |
|    learning_rate        | 0.0003       |
|    loss                 | 374          |
|    n_updates            | 480          |
|    policy_gradient_loss | 0.000214     |
|    value_loss           | 1.22e+03     |
------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 80
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1111111610984565}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984565}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 1
reward: {'Agent_0': 0.9622507125282667}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282667}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 2
reward: {'Agent_0': 0.5555558286463782}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555558286463782}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555558]
这一步的reward[0]: 0.5555558204650879
n_steps: 3
reward: {'Agent_0': 4.880130202167265e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 4.880130202167265e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 4
reward: {'Agent_0': 0.5555543676018893}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555543676018893. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.9622499860966265}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622499860966265. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.1111111610984494}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984494}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 7
reward: {'Agent_0': 0.9622509509468387}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622509509468387. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [1.1111112, 0.9622507, 0.5555558, -10.0, -2.0, -2.0, 1.1111112, -2.0]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 34.5          |
|    ep_rew_mean          | -87.40989     |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 11            |
|    time_elapsed         | 2             |
|    total_timesteps      | 88            |
| train/                  |               |
|    approx_kl            | 2.7962029e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0377       |
|    explained_variance   | 0.87          |
|    learning_rate        | 0.0003        |
|    loss                 | 4.78          |
|    n_updates            | 490           |
|    policy_gradient_loss | 0.00176       |
|    value_loss           | 19.6          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 88
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555564246928242}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555564246928242. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.143664118075094e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.143664118075094e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.5555534139275675}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555534139275675}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555344]
这一步的reward[0]: 0.5555534362792969
n_steps: 3
reward: {'Agent_0': 0.9622498668873334}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622498668873334}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622499]
这一步的reward[0]: 0.9622498750686646
n_steps: 4
reward: {'Agent_0': 1.1111111610984477}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984477}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 5
reward: {'Agent_0': 0.9622507125282631}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282631}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 6
reward: {'Agent_0': 0.5555563054835275}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555563054835275}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555563]
这一步的reward[0]: 0.5555562973022461
n_steps: 7
reward: {'Agent_0': 9.648501801962084e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 9.648501801962084e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -10.0, 0.55555344, 0.9622499, 1.1111112, 0.9622507, 0.5555563, -10.0]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 34.5          |
|    ep_rew_mean          | -87.40989     |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 12            |
|    time_elapsed         | 2             |
|    total_timesteps      | 96            |
| train/                  |               |
|    approx_kl            | 2.2783875e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0121       |
|    explained_variance   | -2.77         |
|    learning_rate        | 0.0003        |
|    loss                 | 18.6          |
|    n_updates            | 500           |
|    policy_gradient_loss | 0.00326       |
|    value_loss           | 80.5          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 96
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.5555534139275693}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555534139275693. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.9622497476780474}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622497476780474. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.1111111610984388}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984388}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 3
reward: {'Agent_0': 0.9622511893654071}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622511893654071. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.5555569015299788}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555569015299788. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.6801059210536096e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.6801059210536096e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.5555524602532493}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555524602532493}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555525]
这一步的reward[0]: 0.5555524826049805
n_steps: 7
reward: {'Agent_0': 0.9622496284687507}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622496284687507}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96224964]
这一步的reward[0]: 0.9622496366500854
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, 1.1111112, -2.0, -2.0, -10.0, 0.5555525, 0.96224964]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 34.5          |
|    ep_rew_mean          | -87.40989     |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 13            |
|    time_elapsed         | 3             |
|    total_timesteps      | 104           |
| train/                  |               |
|    approx_kl            | 3.0845404e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00645      |
|    explained_variance   | -1.76         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.6          |
|    n_updates            | 510           |
|    policy_gradient_loss | 0.000658      |
|    value_loss           | 44.1          |
-------------------------------------------
？？？？？是不是一直在for index in range(config[epochs]):里循环???epoch:4

Training on 1_to_2lane_left_turn_c.

####
Logging to /home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43/tensorboard/PPO_5
?????????????????????有没有在这里发生循环？num_timesteps: 0
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 1
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 2
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.555555947855666}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555947855666. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225053, 0.55555534, -10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0]
---------------------------
| time/              |    |
|    fps             | 26 |
|    iterations      | 1  |
|    time_elapsed    | 0  |
|    total_timesteps | 8  |
---------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 8
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555544868111726}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555544868111726}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555545]
这一步的reward[0]: 0.5555545091629028
n_steps: 2
reward: {'Agent_0': 0.962250105305916}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250105305916}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622501]
这一步的reward[0]: 0.9622501134872437
n_steps: 3
reward: {'Agent_0': 1.1111111610984565}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984565}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622507125282667}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282667}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 5
reward: {'Agent_0': 0.5555558286463782}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555558286463782}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555558]
这一步的reward[0]: 0.5555558204650879
n_steps: 6
reward: {'Agent_0': 4.880130202167265e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 4.880130202167265e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.5555543676018893}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555543676018893. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.5555545, 0.9622501, 1.1111112, 0.9622507, 0.5555558, -10.0, -2.0]
------------------------------------------
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 2            |
|    time_elapsed         | 0            |
|    total_timesteps      | 16           |
| train/                  |              |
|    approx_kl            | 7.450581e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000368    |
|    explained_variance   | -1.35        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.68         |
|    n_updates            | 530          |
|    policy_gradient_loss | 1.87e-05     |
|    value_loss           | 22.2         |
------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 16
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622499860966265}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622499860966265. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.1111111610984494}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984494}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 2
reward: {'Agent_0': 0.9622509509468387}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622509509468387. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555564246928242}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555564246928242. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 1.143664118075094e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.143664118075094e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.5555534139275675}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555534139275675}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555344]
这一步的reward[0]: 0.5555534362792969
n_steps: 6
reward: {'Agent_0': 0.9622498668873334}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622498668873334}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622499]
这一步的reward[0]: 0.9622498750686646
n_steps: 7
reward: {'Agent_0': 1.1111111610984477}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984477}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, 1.1111112, -2.0, -2.0, -10.0, 0.55555344, 0.9622499, 1.1111112]
---------------------------------------
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 3         |
|    time_elapsed         | 0         |
|    total_timesteps      | 24        |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0.325     |
|    learning_rate        | 0.0003    |
|    loss                 | 27.5      |
|    n_updates            | 540       |
|    policy_gradient_loss | 1.24e-05  |
|    value_loss           | 64.4      |
---------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 24
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622507125282631}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282631}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 1
reward: {'Agent_0': 0.5555563054835275}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555563054835275}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555563]
这一步的reward[0]: 0.5555562973022461
n_steps: 2
reward: {'Agent_0': 9.648501801962084e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 9.648501801962084e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555534139275693}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555534139275693. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622497476780474}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622497476780474. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.1111111610984388}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984388}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622511893654071}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622511893654071. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.5555569015299788}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555569015299788. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.9622507, 0.5555563, -10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0]
-------------------------------------------
| time/                   |               |
|    fps                  | 34            |
|    iterations           | 4             |
|    time_elapsed         | 0             |
|    total_timesteps      | 32            |
| train/                  |               |
|    approx_kl            | -7.450581e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000113     |
|    explained_variance   | 0.793         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.83          |
|    n_updates            | 550           |
|    policy_gradient_loss | -1.89e-06     |
|    value_loss           | 12.5          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 32
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.6801059210536096e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.6801059210536096e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555524602532493}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555524602532493}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555525]
这一步的reward[0]: 0.5555524826049805
n_steps: 2
reward: {'Agent_0': 0.9622496284687507}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622496284687507}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96224964]
这一步的reward[0]: 0.9622496366500854
n_steps: 3
reward: {'Agent_0': 1.1111111610984388}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984388}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622514277839826}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622514277839826}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622514]
这一步的reward[0]: 0.9622514247894287
n_steps: 5
reward: {'Agent_0': 0.5555567823206928}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555567823206928}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555568]
这一步的reward[0]: 0.5555567741394043
n_steps: 6
reward: {'Agent_0': 1.9185244966024584e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.9185244966024584e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.5555519834160982}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555519834160982. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.5555525, 0.96224964, 1.1111112, 0.9622514, 0.5555568, -10.0, -2.0]
--------------------------------------------
| time/                   |                |
|    fps                  | 34             |
|    iterations           | 5              |
|    time_elapsed         | 1              |
|    total_timesteps      | 40             |
| train/                  |                |
|    approx_kl            | -1.4901161e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.57e-05      |
|    explained_variance   | -1.19          |
|    learning_rate        | 0.0003         |
|    loss                 | 7.2            |
|    n_updates            | 560            |
|    policy_gradient_loss | 1.4e-06        |
|    value_loss           | 32.2           |
--------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 40
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622495092594612}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622495092594612. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 1.1111111610984317}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984317}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 2
reward: {'Agent_0': 0.9622514277839791}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622514277839791. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.5555573187624887}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555573187624887. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 2.1569430792567346e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 2.1569430792567346e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.5555515661835777}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555515661835777}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555516]
这一步的reward[0]: 0.5555515885353088
n_steps: 6
reward: {'Agent_0': 0.9622492708408856}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622492708408856}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622493]
这一步的reward[0]: 0.9622492790222168
n_steps: 7
reward: {'Agent_0': 1.1111111610984281}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984281}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, 1.1111112, -2.0, -2.0, -10.0, 0.5555516, 0.9622493, 1.1111112]
---------------------------------------
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 6         |
|    time_elapsed         | 1         |
|    total_timesteps      | 48        |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.77e-05 |
|    explained_variance   | 0.688     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.81      |
|    n_updates            | 570       |
|    policy_gradient_loss | 3.35e-07  |
|    value_loss           | 14.8      |
---------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 48
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622516662025546}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622516662025546}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225166]
这一步的reward[0]: 0.9622516632080078
n_steps: 1
reward: {'Agent_0': 0.5555572591578439}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555572591578439}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555725]
这一步的reward[0]: 0.5555572509765625
n_steps: 2
reward: {'Agent_0': 2.3953616619110107e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 2.3953616619110107e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555510297417783}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555510297417783. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 0.9622492708408785}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622492708408785. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 1.111111161098421}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111111161098421}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622516662025475}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622516662025475. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 0.5555577955996469}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555577955996469. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.96225166, 0.55555725, -10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0]
---------------------------------------
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 7         |
|    time_elapsed         | 1         |
|    total_timesteps      | 56        |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.73e-05 |
|    explained_variance   | -1.27     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.1      |
|    n_updates            | 580       |
|    policy_gradient_loss | 7.17e-07  |
|    value_loss           | 48.6      |
---------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 56
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 2.693384885787964e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 2.693384885787964e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555504932999398}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555504932999398}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555505]
这一步的reward[0]: 0.5555505156517029
n_steps: 2
reward: {'Agent_0': 0.9622491516316245}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622491516316245}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96224916]
这一步的reward[0]: 0.9622491598129272
n_steps: 3
reward: {'Agent_0': 1.111111161098421}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.111111161098421}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622519046211266}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622519046211266}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622519]
这一步的reward[0]: 0.9622519016265869
n_steps: 5
reward: {'Agent_0': 0.5555577359950021}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Collided. Penalty of collisions = 10. Agent reward: 0.5555577359950021. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 6
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
n_steps: 7
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, 0.5555505, 0.96224916, 1.1111112, 0.9622519, -10.0, 0.96225053, 0.55555534]
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 62         |
|    ep_rew_mean          | -122.74858 |
| time/                   |            |
|    fps                  | 32         |
|    iterations           | 8          |
|    time_elapsed         | 1          |
|    total_timesteps      | 64         |
| train/                  |            |
|    approx_kl            | 0.0        |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13e-06  |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.2       |
|    n_updates            | 590        |
|    policy_gradient_loss | 6.71e-08   |
|    value_loss           | 61.4       |
----------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 64
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.555555947855666}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555947855666. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.5555544868111726}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555544868111726}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555545]
这一步的reward[0]: 0.5555545091629028
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0, -10.0, 0.5555545]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 62            |
|    ep_rew_mean          | -122.74858    |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 9             |
|    time_elapsed         | 2             |
|    total_timesteps      | 72            |
| train/                  |               |
|    approx_kl            | 1.4901161e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.7e-05      |
|    explained_variance   | -0.0641       |
|    learning_rate        | 0.0003        |
|    loss                 | 940           |
|    n_updates            | 600           |
|    policy_gradient_loss | 7.3e-07       |
|    value_loss           | 3.57e+03      |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 72
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.962250105305916}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.962250105305916}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622501]
这一步的reward[0]: 0.9622501134872437
n_steps: 1
reward: {'Agent_0': 1.1111111610984565}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984565}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 2
reward: {'Agent_0': 0.9622507125282667}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282667}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 3
reward: {'Agent_0': 0.5555558286463782}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555558286463782}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555558]
这一步的reward[0]: 0.5555558204650879
n_steps: 4
reward: {'Agent_0': 4.880130202167265e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 4.880130202167265e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 5
reward: {'Agent_0': 0.5555543676018893}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555543676018893. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.9622499860966265}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622499860966265. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 1.1111111610984494}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984494}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.9622501, 1.1111112, 0.9622507, 0.5555558, -10.0, -2.0, -2.0, 1.1111112]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 62            |
|    ep_rew_mean          | -122.74858    |
| time/                   |               |
|    fps                  | 31            |
|    iterations           | 10            |
|    time_elapsed         | 2             |
|    total_timesteps      | 80            |
| train/                  |               |
|    approx_kl            | 2.9802322e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00232      |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.0003        |
|    loss                 | 16.3          |
|    n_updates            | 610           |
|    policy_gradient_loss | 6.87e-05      |
|    value_loss           | 60.1          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 80
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.9622509509468387}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622509509468387. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 1
reward: {'Agent_0': 0.5555564246928242}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555564246928242. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 1.143664118075094e-06}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.143664118075094e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 3
reward: {'Agent_0': 0.5555534139275675}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555534139275675}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555344]
这一步的reward[0]: 0.5555534362792969
n_steps: 4
reward: {'Agent_0': 0.9622498668873334}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622498668873334}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622499]
这一步的reward[0]: 0.9622498750686646
n_steps: 5
reward: {'Agent_0': 1.1111111610984477}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984477}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 6
reward: {'Agent_0': 0.9622507125282631}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622507125282631}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.9622507]
这一步的reward[0]: 0.9622507095336914
n_steps: 7
reward: {'Agent_0': 0.5555563054835275}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.5555563054835275}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.5555563]
这一步的reward[0]: 0.5555562973022461
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-2.0, -2.0, -10.0, 0.55555344, 0.9622499, 1.1111112, 0.9622507, 0.5555563]
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 62            |
|    ep_rew_mean          | -122.74858    |
| time/                   |               |
|    fps                  | 31            |
|    iterations           | 11            |
|    time_elapsed         | 2             |
|    total_timesteps      | 88            |
| train/                  |               |
|    approx_kl            | 1.4901161e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000605     |
|    explained_variance   | -0.934        |
|    learning_rate        | 0.0003        |
|    loss                 | 18.3          |
|    n_updates            | 620           |
|    policy_gradient_loss | 7.06e-05      |
|    value_loss           | 54.4          |
-------------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 88
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 9.648501801962084e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 9.648501801962084e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 1
reward: {'Agent_0': 0.5555534139275693}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555534139275693. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 2
reward: {'Agent_0': 0.9622497476780474}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622497476780474. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 1.1111111610984388}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984388}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 4
reward: {'Agent_0': 0.9622511893654071}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622511893654071. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 5
reward: {'Agent_0': 0.5555569015299788}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555569015299788. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 1.6801059210536096e-06}
done: {'Agent_0': True, '__all__': True}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Collided. Penalty of collisions = 10. Agent reward: 1.6801059210536096e-06. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 7
reward: {'Agent_0': 0.9622505375886288}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.9622505375886288}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.96225053]
这一步的reward[0]: 0.9622505307197571
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [-10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0, -10.0, 0.96225053]
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 47.5      |
|    ep_rew_mean          | -96.802   |
| time/                   |           |
|    fps                  | 30        |
|    iterations           | 12        |
|    time_elapsed         | 3         |
|    total_timesteps      | 96        |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000546 |
|    explained_variance   | 0.732     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 630       |
|    policy_gradient_loss | 7.27e-06  |
|    value_loss           | 42.7      |
---------------------------------------
?????????????????????有没有在这里发生循环？num_timesteps: 96
n_rollout_steps: 8
n_steps: 0
reward: {'Agent_0': 0.555555351809236}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 0.555555351809236}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [0.55555534]
这一步的reward[0]: 0.5555553436279297
n_steps: 1
reward: {'Agent_0': 1.1175862901779965e-08}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 1.1175862901779965e-08. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
n_steps: 2
reward: {'Agent_0': 0.5555553212762065}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.5555553212762065. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 3
reward: {'Agent_0': 0.9622502245152083}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622502245152083. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 4
reward: {'Agent_0': 1.1111111610984574}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
wrapped_reward: {'Agent_0': 1.1111111610984574}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [1.1111112]
这一步的reward[0]: 1.1111111640930176
n_steps: 5
reward: {'Agent_0': 0.9622505933189798}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.9622505933189798. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 6
reward: {'Agent_0': 0.555555947855666}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went on shoulder. Penalty of on_shoulder = 2. Agent reward: 0.555555947855666. reward[agent_id]: -2.0
wrapped_reward: {'Agent_0': -2.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-2.]
这一步的reward[0]: -2.0
n_steps: 7
reward: {'Agent_0': 6.072223133202215e-07}
done: {'Agent_0': False, '__all__': False}
length of env_reward: 1
length of env_reward.items(): 1
type of reward: <class 'dict'>
reward: {'Agent_0': 0.0}
Agent_0: Went wrong way. Penalty of wrong_way = 10. Agent reward: 6.072223133202215e-07. reward[agent_id]: -10.0
wrapped_reward: {'Agent_0': -10.0}
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~in circle: for agent_id, agent_done in done.items():~~~~~~~~~~~~~~~~~~~~
type of rewards: <class 'numpy.ndarray'>
这一步的reward: [-10.]
这一步的reward[0]: -10.0
!!!!!!!!!!!!!!!out of n_steps circle!!!!!!!!!!!!!!!!!!
length of reward_change: 8
reward_change: [0.55555534, -10.0, -2.0, -2.0, 1.1111112, -2.0, -2.0, -10.0]
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 47.5         |
|    ep_rew_mean          | -96.802      |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 13           |
|    time_elapsed         | 3            |
|    total_timesteps      | 104          |
| train/                  |              |
|    approx_kl            | 7.450581e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000254    |
|    explained_variance   | 0.0815       |
|    learning_rate        | 0.0003       |
|    loss                 | 571          |
|    n_updates            | 640          |
|    policy_gradient_loss | 1.78e-06     |
|    value_loss           | 1.92e+03     |
------------------------------------------
/home/bit/software/anaconda3/envs/huawei-smart/lib/python3.7/site-packages/stable_baselines3/common/base_class.py:818: ResourceWarning: unclosed file <_io.BufferedWriter name='/home/bit/SMARTS/competition/track1/train/logs/2022_10_07_16_03_43/train/model_2022_10_07_16_07_04.zip'>
  save_to_zip_file(path, data=data, params=params_to_save, pytorch_variables=pytorch_variables)
ResourceWarning: Enable tracemalloc to get the object allocation traceback

Saved trained model.

